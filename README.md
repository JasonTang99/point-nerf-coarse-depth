# Enhancing Point-NeRF with Stereoscopic Depth Sensors

For a more detailed description of the project, please see the [final report](report/report.pdf).

## Summary
Neural Radiance Fields (NeRF) have recently gained attention for generating realistic 3D scenes from 2D images. However, NeRF's computational demands often hinder its practical applications. To address this challenge, we explored Point-NeRF, an extension that selectively samples points near an estimated point cloud. We conducted empirical experiments using stereoscopic depth sensors (Intel RealSense D435) for coarse point cloud initialization and compared it with alternative methods. Our findings reveal that incorporating stereoscopic depth sensors into the 3D scene reconstruction pipeline enhances novel view image quality while retaining the sample efficiency advantages of Point-NeRF.

<!-- cams -->
#### Camera Locations
![Alt text](report/images/cams.png?raw=true "Real-World Experiments")

## Experiments
Real-world scenes ("books" and "bear") captured using a depth sensor were processed through Point-NeRF, yielding comparable results in generating new views without the time-intensive MVSNet initialization. While the MVSNet initialization method outperforms ours in PSNR and SSIM, our method performs competitively in LPIPS, a more advanced metric approximating human similarity perception. Despite hardware limitations, our approach demonstrates potential for enhancing Point-NeRF's efficiency and performance in 3D scene reconstruction. Challenges encountered include alignment issues and object segmentation failures.

<!-- books -->
#### Novel Generated Views - Books Scene
![Alt text](report/images/books.png?raw=true "Books")

<!-- bear -->
#### Novel Generated Views - Bear Scene
![Alt text](report/images/bear.png?raw=true "Bear")

## Conclusions
The integration of stereoscopic depth sensors to initialize point clouds in Point-NeRF holds promise for elevating novel view image quality and streamlining preprocessing time. Future endeavors will focus on hyperparameter optimization, broader method comparisons across diverse scenes, and investigating the fusion of depth images into multiview stereo techniques.

<!-- loss -->
#### Metrics over training iterations
![Alt text](report/images/loss.png?raw=true "Loss")


<!-- ############################################################################################ -->
## File Structure
* ```data/``` - the data collected from the real-world experiments. (Not uploaded to GitHub due to size, see below.)
* ```misc/``` - old scripts and notebooks used for data collection and analysis.
* ```plot_imgs/``` - images used in the report.
* ```pointnerf/``` - code from the [official Point-NeRF Implementation](https://github.com/Xharlie/pointnerf). Some of the code is modified to run our experiments.
* ```pointnerf/checkpoints/``` - checkpointed run data from the experiments. (Not uploaded to GitHub due to size, see below.)
* ```pointnerf/data_src/``` - processed data used to train the Point-NeRF model. (Not uploaded to GitHub due to size, see below.)
* ```pointnerf/scripts/``` - scripts to run our experiments.
* ```multiway_reg.py``` - code to run the multiway registration.
* ```plot.ipynb``` - notebook to plot the results shown in the report.
* ```pose_bear.ipynb``` - (main code) notebook to run the pose estimation on the bear scene. Contains COLMAP pose processing, checkerboard pose processing, image preprocessing, point cloud preprocessing, and multiway registration.
* ```process_bag.py``` - code to process the rosbag files generated by the Intel RealSense D435 Depth Camera. 
* ```utils.py``` - utility functions used in the notebooks.

## Data
The collected data is available at this [Google Drive Link](https://drive.google.com/file/d/1GoUAKg_cYg8mOlF4c6UzspSZBblQxR9W/view?usp=sharing). Unzip and leave the "data" folder in this directory.

The checkpointed data from the experiments are available at this [Google Drive Link](https://drive.google.com/file/d/12V81zXsEQGoPJjw1pRJ4FXC-id6qfmXd/view?usp=sharing). Unzip and move the "checkpoints" and "data_src" folders to the ```pointnerf/``` directory.

## Libraries required
Run:
```pip install -r requirements.txt```
and follow the next section to setup Point-NeRF.

## Setup Point-NeRF
Follow the setup required in [official Point-NeRF Implementation](https://github.com/Xharlie/pointnerf).

The scripts to run our experiments are under ```pointnerf/scripts```:

* ```bash bear.sh``` - runs the bear scene using MVSNet.
* ```bash real_bear.sh``` - runs the bear scene using depth sensor initialized pointcloud.
* ```bash books.sh``` - runs the books scene using MVSNet.
* ```bash real_books.sh``` - runs the books scene using depth sensor initialized pointcloud.


## Note on running Point-NeRF experiments
The code on this repository is built on top of the [official Point-NeRF Implementation](https://github.com/Xharlie/pointnerf), which was built using [PyCUDA](https://wiki.tiker.net/PyCuda/Installation/), which is not up to date and is difficult to set up. Here are my setup details which is able to run:
* Ubuntu 22.04.2 LTS (64-bit)
* NVIDIA GeForce RTX 3060 Ti (8GB VRAM)
* NVIDIA-SMI 515.105.01
* Driver Version: 515.105.01
* CUDA Version: 11.7
* NVIDIA Cuda Compiler (nvcc) V11.7.64

